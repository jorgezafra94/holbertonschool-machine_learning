{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bayesoptimization",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDAREgX5kOBh",
        "colab_type": "text"
      },
      "source": [
        "Import Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryr3PyYqhPCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modules\n",
        "%reload_ext tensorboard\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cNZxKOtozWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9a46585f-5845-424c-cb6e-140e6db33e8f"
      },
      "source": [
        "# install GPy and GPyOpt\n",
        "!pip install GPy\n",
        "!pip install GPyOpt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPy in /usr/local/lib/python3.6/dist-packages (1.9.9)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from GPy) (0.9.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from GPy) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from GPy) (1.18.5)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Requirement already satisfied: GPyOpt in /usr/local/lib/python3.6/dist-packages (1.2.6)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.4.1)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.9.9)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->GPyOpt) (1.15.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->GPyOpt) (0.9.5)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et44pDbhxWhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import GPy Module\n",
        "import GPy\n",
        "import GPyOpt\n",
        "from GPyOpt.methods import BayesianOptimization"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWNjTy0kTfu",
        "colab_type": "text"
      },
      "source": [
        "LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkBvMEcni0uk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "90b0af1e-86f9-4c01-b473-ea354de181a6"
      },
      "source": [
        "# setting the train and test data \n",
        "# datasets = mnist.load_data()\n",
        "datasets = np.load('../content/MNIST.npz')\n",
        "X_train = datasets['X_train']\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "Y_train = datasets['Y_train']\n",
        "X_valid = datasets['X_valid']\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
        "Y_valid = datasets['Y_valid']\n",
        "\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(X_train.shape))\n",
        "print(\"- Validation-set:\\t{}\".format(X_valid.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of:\n",
            "- Training-set:\t\t(50000, 784)\n",
            "- Validation-set:\t(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnE2xoY3kWc7",
        "colab_type": "text"
      },
      "source": [
        "ONE HOT FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ8wmuLYi6Ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(labels, classes=None):\n",
        "    \"\"\"\n",
        "    that converts a label vector into a one-hot matrix:\n",
        "\n",
        "    The last dimension of the one-hot matrix must be the number of classes\n",
        "    Returns: the one-hot matrix\n",
        "    \"\"\"\n",
        "    one_hot = K.utils.to_categorical(y=labels, num_classes=classes)\n",
        "    return one_hot"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVDLo9ljJWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "313fdb18-2d86-48b9-ffd5-041fc65b1c51"
      },
      "source": [
        "Y_train_oh = one_hot(Y_train)\n",
        "Y_valid_oh = one_hot(Y_valid)\n",
        "\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(Y_train.shape))\n",
        "print(\"- Validation-set:\\t{}\".format(Y_valid.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of:\n",
            "- Training-set:\t\t(50000,)\n",
            "- Validation-set:\t(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXg7rXC2kZ23",
        "colab_type": "text"
      },
      "source": [
        "MODEL STRUCTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUow_it3jM6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
        "    \"\"\"\n",
        "    * nx is the number of input features to the network\n",
        "    * layers is a list containing the number of nodes in each layer of the\n",
        "        network\n",
        "    * activations is a list containing the activation functions used for each\n",
        "        layer of the network\n",
        "    * lambtha is the L2 regularization parameter\n",
        "    * keep_prob is the probability that a node will be kept for dropout\n",
        "    * You are not allowed to use the Input class\n",
        "    * Returns: the keras model\n",
        "    \"\"\"\n",
        "    reg = K.regularizers.l2(lambtha)\n",
        "\n",
        "    X = K.Input(shape=(nx,))\n",
        "    layer_l2 = K.layers.Dense(units=layers[0], activation=activations[0],\n",
        "                              kernel_regularizer=reg)\n",
        "    Y_prev = layer_l2(X)\n",
        "\n",
        "    for i in range(1, len(layers)):\n",
        "\n",
        "        layer_drop = K.layers.Dropout(1 - keep_prob)\n",
        "        Y = layer_drop(Y_prev)\n",
        "\n",
        "        layer_l2 = K.layers.Dense(units=layers[i], activation=activations[i],\n",
        "                                  kernel_regularizer=reg)\n",
        "        Y_prev = layer_l2(Y)\n",
        "\n",
        "    Y_pred = Y_prev\n",
        "    model = K.Model(inputs=X, outputs=Y_pred)\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M_KD1JUkdkS",
        "colab_type": "text"
      },
      "source": [
        "MODEL OPTIMIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsPYzhhJj_VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model(network, alpha, beta1, beta2):\n",
        "    \"\"\"\n",
        "    * that sets up Adam optimization for a keras model with\n",
        "         categorical crossentropy loss and accuracy metrics:\n",
        "    * network is the model to optimize\n",
        "    * alpha is the learning rate\n",
        "    * beta1 is the first Adam optimization parameter\n",
        "    * beta2 is the second Adam optimization parameter\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    Adam = K.optimizers.Adam(lr=alpha, beta_1=beta1, beta_2=beta2)\n",
        "    network.compile(loss='categorical_crossentropy', optimizer=Adam,\n",
        "                    metrics=['accuracy'])\n",
        "    return None"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y06BHoyRmPxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(network, data, labels, batch_size, epochs,\n",
        "                validation_data=None, early_stopping=False, patience=0,\n",
        "                learning_rate_decay=False, alpha=0.1, decay_rate=1,\n",
        "                save_best=False, filepath=None, verbose=True, shuffle=False):\n",
        "    \"\"\"\n",
        "    * network is the model to train\n",
        "    * data is a numpy.ndarray of shape (m, nx) containing the input data\n",
        "    * labels is a one-hot numpy.ndarray of shape (m, classes) containing\n",
        "            the labels of data\n",
        "    * batch_size is the size of the batch used for mini-batch gradient descent\n",
        "    * epochs is the number of passes through data for mini-batch\n",
        "             gradient descent\n",
        "    * verbose is a boolean that determines if output should be printed during\n",
        "            training\n",
        "    * shuffle is a boolean that determines whether to shuffle the batches every\n",
        "          epoch. Normally, it is a good idea to shuffle, but for\n",
        "          reproducibility we have chosen to set the default to False.\n",
        "    * early_stopping is a boolean that indicates whether early stopping\n",
        "          should be used\n",
        "          early stopping should only be performed if validation_data exists\n",
        "          early stopping should be based on validation loss\n",
        "    * patience is the patience used for early stopping\n",
        "    * learning_rate_decay is a boolean that indicates whether learning rate\n",
        "          decay should be used\n",
        "      learning rate decay should only be performed if validation_data exists\n",
        "      the decay should be performed using inverse time decay\n",
        "      the learning rate should decay in a stepwise fashion after each epoch\n",
        "    * each time the learning rate updates, Keras should print a message\n",
        "           alpha is the initial learning rate\n",
        "           decay_rate is the decay rate\n",
        "    * save_best is a boolean indicating whether to save the model after each\n",
        "           epoch if it is the best\n",
        "    * a model is considered the best if its validation loss is the lowest that\n",
        "           the model has obtained\n",
        "    * filepath is the file path where the model should be saved\n",
        "    Returns: the History object generated after training the model\n",
        "    \"\"\"\n",
        "\n",
        "    def learning_decay(epoch):\n",
        "        \"\"\"\n",
        "        funcion in the learningRateSchedule\n",
        "        the alpha doesnt change\n",
        "        \"\"\"\n",
        "        return alpha / (1 + decay_rate * (epoch / 1))\n",
        "\n",
        "    my_list = []\n",
        "    if early_stopping is True and validation_data:\n",
        "        early = K.callbacks.EarlyStopping(patience=patience)\n",
        "        my_list.append(early)\n",
        "\n",
        "    if learning_rate_decay is True and validation_data:\n",
        "        learn_dec = K.callbacks.LearningRateScheduler(learning_decay,\n",
        "                                                      verbose=1)\n",
        "        my_list.append(learn_dec)\n",
        "\n",
        "    if save_best is True and validation_data:\n",
        "        save = K.callbacks.ModelCheckpoint(filepath=filepath,\n",
        "                                           save_best_only=True)\n",
        "        my_list.append(save)\n",
        "\n",
        "    if len(my_list) == 0:\n",
        "        my_list = None\n",
        "\n",
        "    history = network.fit(x=data, y=labels, batch_size=batch_size,\n",
        "                          epochs=epochs, verbose=verbose, shuffle=shuffle,\n",
        "                          validation_data=validation_data, callbacks=my_list)\n",
        "    \n",
        "    return history\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF85fd7sxaUh",
        "colab_type": "text"
      },
      "source": [
        "FUNCTIONS DEFINITIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h4HjA4imdjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_accuracy = 0\n",
        "\n",
        "def F(l2, lr, drop, unit, bch_size):\n",
        "    print('Input', l2, lr, drop, unit, bch_size)\n",
        "    lambtha = l2\n",
        "    keep_prob = drop\n",
        "    network = build_model(784, [int(unit), int(unit), 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
        "    alpha = lr\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    optimize_model(network, alpha, beta1, beta2)\n",
        "    batch_size = int(bch_size)\n",
        "    epochs = 5\n",
        "    \n",
        "    filepath = 'model_lambda{}_keepprob{}_batch{}_units{}_learningr{}.h5'.format(lambtha, keep_prob, batch_size, int(unit), lr)\n",
        "\n",
        "    history = train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
        "                          validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
        "                          patience=3, learning_rate_decay=True, alpha=alpha,\n",
        "                          save_best=True, filepath=filepath)\n",
        "   \n",
        "\n",
        "    # Get the classification accuracy on the validation-set\n",
        "    # after the last training-epoch.\n",
        "    global best_accuracy\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "    # Print the classification accuracy.\n",
        "    print()\n",
        "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
        "    print()\n",
        "\n",
        "    # Save the model if it improves on the best-found performance.\n",
        "    # We use the global keyword so we update the variable outside\n",
        "    # of this function.\n",
        "    \n",
        "    print(accuracy, best_accuracy)\n",
        "    path_best_model='best.h5'\n",
        "\n",
        "    # If the classification accuracy of the saved model is improved ...\n",
        "    if accuracy > best_accuracy:\n",
        "        # Save the new model to harddisk.\n",
        "        network.save(path_best_model)\n",
        "        \n",
        "        # Update the classification accuracy.\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "    # Delete the Keras model with these hyper-parameters from memory.\n",
        "    del network\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def f(X):\n",
        "  Y = np.zeros((X.shape[0], 1))\n",
        "  for i in range(X.shape[0]):\n",
        "    Y[i] = F(X[i][0], X[i][1], X[i][2], X[i][3], X[i][4])\n",
        "    \n",
        "  return(Y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S86-d9D5xtCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "e6fbffdb-6388-4048-917b-dc7bd1317967"
      },
      "source": [
        "\n",
        "X_init = np.array([[0.001, 0.0001, 0.015, 448, 128], [0.00809, 0.00624, 0.42, 500, 16]])\n",
        "Y_init = f(X_init)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input 0.001 0.0001 0.015 448.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 7.2735 - accuracy: 0.0957 - val_loss: 2.8966 - val_accuracy: 0.1149\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 5e-05.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 3.2575 - accuracy: 0.0993 - val_loss: 2.8436 - val_accuracy: 0.1084\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 3.3333333333333335e-05.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 3.0467 - accuracy: 0.1029 - val_loss: 2.8246 - val_accuracy: 0.1083\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 2.5e-05.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 2.9570 - accuracy: 0.1043 - val_loss: 2.8149 - val_accuracy: 0.1068\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2e-05.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 2.9174 - accuracy: 0.1046 - val_loss: 2.8088 - val_accuracy: 0.1067\n",
            "\n",
            "Accuracy: 10.67%\n",
            "\n",
            "0.10670000314712524 0\n",
            "Input 0.00809 0.00624 0.42 500.0 16.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.00624.\n",
            "Epoch 1/5\n",
            "3125/3125 [==============================] - 23s 7ms/step - loss: 2.0176 - accuracy: 0.6068 - val_loss: 1.2944 - val_accuracy: 0.8273\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00312.\n",
            "Epoch 2/5\n",
            "3125/3125 [==============================] - 24s 8ms/step - loss: 1.4895 - accuracy: 0.6492 - val_loss: 1.0509 - val_accuracy: 0.8572\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00208.\n",
            "Epoch 3/5\n",
            "3125/3125 [==============================] - 23s 7ms/step - loss: 1.3475 - accuracy: 0.6933 - val_loss: 0.9548 - val_accuracy: 0.8813\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00156.\n",
            "Epoch 4/5\n",
            "3125/3125 [==============================] - 23s 7ms/step - loss: 1.2777 - accuracy: 0.7135 - val_loss: 0.8955 - val_accuracy: 0.8755\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001248.\n",
            "Epoch 5/5\n",
            "3125/3125 [==============================] - 23s 7ms/step - loss: 1.2373 - accuracy: 0.7311 - val_loss: 0.8589 - val_accuracy: 0.8911\n",
            "\n",
            "Accuracy: 89.11%\n",
            "\n",
            "0.8910999894142151 0.10670000314712524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WZ-pExa2s4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a4798a64-d491-442f-a178-0cdb966cd74c"
      },
      "source": [
        "print(Y_init)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1067    ]\n",
            " [0.89109999]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVvuQIXh35kZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28c5325d-c69b-4178-c725-35bf0c2767a8"
      },
      "source": [
        "# kernel = GPy.kern.Matern52(input_dim=1, variance=1.0, lengthscale=1.0)\n",
        "bds = [{'name': 'l2', 'type': 'continuous', 'domain': (0.00001, 0.01)},\n",
        "       {'name': 'lr', 'type': 'continuous', 'domain': (0.00001, 0.01)}, # added 1 0 at min\n",
        "       {'name': 'drop', 'type': 'continuous', 'domain': (0, 0.5)},\n",
        "       {'name': 'unit', 'type': 'discrete', 'domain': (64, 512)},\n",
        "       {'name': 'bch_size', 'type': 'discrete', 'domain': (16, 128)}]\n",
        "\n",
        "optimizer = BayesianOptimization(f=f, \n",
        "                                 domain=bds,\n",
        "                                 model_type='GP',\n",
        "                                 #kernel=kernel,\n",
        "                                 acquisition_type ='EI',\n",
        "                                 acquisition_jitter = 0.01,\n",
        "                                 X=X_init,\n",
        "                                 Y=Y_init,\n",
        "                                 exact_feval=False,\n",
        "                                 normalize_Y=False,\n",
        "                                 maximize=True,\n",
        "                                 report_file='bayes_opt.txt')\n",
        "\n",
        "\n",
        "optimizer.run_optimization(max_iter=9)\n",
        "optimizer.plot_acquisition()\n",
        "optimizer.plot_convergence()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input 0.009769954830299452 0.008494288476183016 0.06158158082087262 64.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.008494288476183016.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 2.4628 - accuracy: 0.1130 - val_loss: 2.3035 - val_accuracy: 0.1064\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.004247144238091508.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.3019 - accuracy: 0.1136 - val_loss: 2.3019 - val_accuracy: 0.1064\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.002831429492061005.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.3013 - accuracy: 0.1136 - val_loss: 2.3018 - val_accuracy: 0.1064\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.002123572119045754.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.3012 - accuracy: 0.1136 - val_loss: 2.3019 - val_accuracy: 0.1064\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0016988576952366031.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.3011 - accuracy: 0.1136 - val_loss: 2.3019 - val_accuracy: 0.1064\n",
            "\n",
            "Accuracy: 10.64%\n",
            "\n",
            "0.10639999806880951 0.9509999752044678\n",
            "Input 0.0033144414581439655 0.006250346188123396 0.4886463319191846 64.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.006250346188123396.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 1.1914 - accuracy: 0.7315 - val_loss: 0.6585 - val_accuracy: 0.9125\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.003125173094061698.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.9031 - accuracy: 0.8191 - val_loss: 0.5683 - val_accuracy: 0.9237\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020834487293744652.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.8464 - accuracy: 0.8330 - val_loss: 0.5392 - val_accuracy: 0.9292\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001562586547030849.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.8193 - accuracy: 0.8395 - val_loss: 0.5256 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0012500692376246792.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.7941 - accuracy: 0.8472 - val_loss: 0.5183 - val_accuracy: 0.9324\n",
            "\n",
            "Accuracy: 93.24%\n",
            "\n",
            "0.9323999881744385 0.9509999752044678\n",
            "Input 0.01 1e-05 0.3796584443423319 64.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 4.3939 - accuracy: 0.1047 - val_loss: 4.1707 - val_accuracy: 0.1533\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 5e-06.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 4.2295 - accuracy: 0.1165 - val_loss: 4.0976 - val_accuracy: 0.2093\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 3.3333333333333337e-06.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 4.1539 - accuracy: 0.1263 - val_loss: 4.0508 - val_accuracy: 0.2505\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 2.5e-06.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 4.1099 - accuracy: 0.1332 - val_loss: 4.0162 - val_accuracy: 0.2801\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 4.0755 - accuracy: 0.1374 - val_loss: 3.9887 - val_accuracy: 0.3015\n",
            "\n",
            "Accuracy: 30.15%\n",
            "\n",
            "0.30149999260902405 0.9509999752044678\n",
            "Input 1e-05 0.01 0.5 64.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.7877 - accuracy: 0.7622 - val_loss: 0.2984 - val_accuracy: 0.9275\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.005.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.8533 - val_loss: 0.2324 - val_accuracy: 0.9425\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0033333333333333335.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.4703 - accuracy: 0.8690 - val_loss: 0.2086 - val_accuracy: 0.9475\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0025.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.4356 - accuracy: 0.8799 - val_loss: 0.2049 - val_accuracy: 0.9508\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.002.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.4213 - accuracy: 0.8826 - val_loss: 0.2003 - val_accuracy: 0.9508\n",
            "\n",
            "Accuracy: 95.08%\n",
            "\n",
            "0.9508000016212463 0.9509999752044678\n",
            "Input 0.00829320486480246 0.0013275754991799752 0.046361508425338704 512.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0013275754991799752.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 4.0920 - accuracy: 0.1870 - val_loss: 2.2621 - val_accuracy: 0.7141\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0006637877495899876.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 2.2105 - accuracy: 0.3710 - val_loss: 1.6333 - val_accuracy: 0.7780\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004425251663933251.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 1.9211 - accuracy: 0.4494 - val_loss: 1.4005 - val_accuracy: 0.7868\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0003318938747949938.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 1.7783 - accuracy: 0.5052 - val_loss: 1.2822 - val_accuracy: 0.8110\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00026551509983599507.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 1.7057 - accuracy: 0.5296 - val_loss: 1.2221 - val_accuracy: 0.8296\n",
            "\n",
            "Accuracy: 82.96%\n",
            "\n",
            "0.8295999765396118 0.9509999752044678\n",
            "Input 0.00741097284894472 0.0017914984172728962 0.17911224688946503 512.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0017914984172728962.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 8s 19ms/step - loss: 2.3822 - accuracy: 0.7139 - val_loss: 1.0444 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0008957492086364481.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 1.1377 - accuracy: 0.8275 - val_loss: 0.8387 - val_accuracy: 0.9166\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0005971661390909654.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 1.0167 - accuracy: 0.8492 - val_loss: 0.7745 - val_accuracy: 0.9219\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00044787460431822406.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.9682 - accuracy: 0.8550 - val_loss: 0.7478 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00035829968345457925.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.9423 - accuracy: 0.8587 - val_loss: 0.7304 - val_accuracy: 0.9250\n",
            "\n",
            "Accuracy: 92.50%\n",
            "\n",
            "0.925000011920929 0.9509999752044678\n",
            "Input 1e-05 0.01 0.33450258210924294 512.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.9032 - accuracy: 0.7614 - val_loss: 0.4277 - val_accuracy: 0.8998\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.005.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.6596 - accuracy: 0.8371 - val_loss: 0.3267 - val_accuracy: 0.9351\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0033333333333333335.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.5047 - accuracy: 0.8794 - val_loss: 0.3045 - val_accuracy: 0.9439\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0025.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.4437 - accuracy: 0.8948 - val_loss: 0.2791 - val_accuracy: 0.9490\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.002.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.4074 - accuracy: 0.9026 - val_loss: 0.2777 - val_accuracy: 0.9485\n",
            "\n",
            "Accuracy: 94.85%\n",
            "\n",
            "0.9484999775886536 0.9509999752044678\n",
            "Input 0.01 1e-05 0.5 512.0 128.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "Epoch 1/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 13.1077 - accuracy: 0.2299 - val_loss: 12.2065 - val_accuracy: 0.7241\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 5e-06.\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 11.9833 - accuracy: 0.4532 - val_loss: 11.4494 - val_accuracy: 0.7734\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 3.3333333333333337e-06.\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 11.3977 - accuracy: 0.5378 - val_loss: 10.9636 - val_accuracy: 0.7901\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 2.5e-06.\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 11.0018 - accuracy: 0.5790 - val_loss: 10.6141 - val_accuracy: 0.8021\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 10.7060 - accuracy: 0.6061 - val_loss: 10.3456 - val_accuracy: 0.8095\n",
            "\n",
            "Accuracy: 80.95%\n",
            "\n",
            "0.809499979019165 0.9509999752044678\n",
            "Input 0.0006786101538609966 0.0029668956112427407 0.2247324243290031 512.0 16.0\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0029668956112427407.\n",
            "Epoch 1/5\n",
            "3125/3125 [==============================] - 26s 8ms/step - loss: 1.6037 - accuracy: 0.6988 - val_loss: 1.0332 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0014834478056213703.\n",
            "Epoch 2/5\n",
            "3125/3125 [==============================] - 30s 10ms/step - loss: 1.1648 - accuracy: 0.7990 - val_loss: 0.6679 - val_accuracy: 0.9266\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0009889652037475803.\n",
            "Epoch 3/5\n",
            "3125/3125 [==============================] - 26s 8ms/step - loss: 0.9146 - accuracy: 0.8327 - val_loss: 0.5288 - val_accuracy: 0.9378\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007417239028106852.\n",
            "Epoch 4/5\n",
            "3125/3125 [==============================] - 25s 8ms/step - loss: 0.7961 - accuracy: 0.8491 - val_loss: 0.4786 - val_accuracy: 0.9418\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005933791222485481.\n",
            "Epoch 5/5\n",
            "3125/3125 [==============================] - 26s 8ms/step - loss: 0.7289 - accuracy: 0.8581 - val_loss: 0.4245 - val_accuracy: 0.9441\n",
            "\n",
            "Accuracy: 94.41%\n",
            "\n",
            "0.944100022315979 0.9509999752044678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eljz7mHb6t-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}